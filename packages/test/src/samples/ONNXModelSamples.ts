import { getGlobalModelRepository } from "@workglow/ai";
import { HF_TRANSFORMERS_ONNX, HfTransformersOnnxModelRecord } from "@workglow/ai-provider";

export async function registerHuggingfaceLocalModels(): Promise<void> {
  const onnxModels: HfTransformersOnnxModelRecord[] = [
    {
      model_id: "onnx:Xenova/all-MiniLM-L6-v2:q8",
      title: "All MiniLM L6 V2 384D",
      description: "Xenova/all-MiniLM-L6-v2",
      tasks: ["TextEmbeddingTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "feature-extraction",
        modelPath: "Xenova/all-MiniLM-L6-v2",
        device: "webgpu",
        nativeDimensions: 384,
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/bge-base-en-v1.5:q8",
      title: "BGE Base English V1.5 768D",
      description: "Xenova/bge-base-en-v1.5",
      tasks: ["TextEmbeddingTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "feature-extraction",
        modelPath: "Xenova/bge-base-en-v1.5",
        device: "webgpu",
        nativeDimensions: 768,
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/gte-small:q8",
      title: "GTE Small 384D",
      description: "Xenova/gte-small",
      tasks: ["TextEmbeddingTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "feature-extraction",
        modelPath: "Xenova/gte-small",
        device: "webgpu",
        nativeDimensions: 384,
      },
      metadata: {},
    },
    {
      model_id: "onnx:onnx-community/bert_uncased_L-2_H-128_A-2-ONNX:q8",
      title: "BERT Uncased 128D",
      description: "onnx-community/bert_uncased_L-2_H-128_A-2-ONNX",
      tasks: ["TextEmbeddingTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "feature-extraction",
        modelPath: "onnx-community/bert_uncased_L-2_H-128_A-2-ONNX",
        device: "webgpu",
        nativeDimensions: 128,
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/paraphrase-albert-base-v2:q8",
      title: "Paraphrase ALBERT Base V2 768D",
      description: "Xenova/paraphrase-albert-base-v2",
      tasks: ["TextEmbeddingTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "feature-extraction",
        modelPath: "Xenova/paraphrase-albert-base-v2",
        device: "webgpu",
        nativeDimensions: 768,
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/distilbert-base-uncased-distilled-squad:q8",
      title: "distilbert-base-uncased-distilled-squad",
      description: "Xenova/distilbert-base-uncased-distilled-squad quantized to 8bit",
      tasks: ["TextQuestionAnsweringTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "question-answering",
        modelPath: "Xenova/distilbert-base-uncased-distilled-squad",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/gpt2:q8",
      title: "gpt2",
      description: "Xenova/gpt2 quantized to 8bit",
      tasks: ["TextGenerationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "text-generation",
        modelPath: "Xenova/gpt2",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/Phi-3-mini-4k-instruct:q4f16",
      title: "Phi-3-mini-4k-instruct:q4f16",
      description: "Xenova/Phi-3-mini-4k-instruct quantized to q4f16",
      tasks: ["TextGenerationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "text-generation",
        modelPath: "Xenova/Phi-3-mini-4k-instruct",
        device: "webgpu",
        dType: "q4f16",
        useExternalDataFormat: true,
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/distilgpt2:q8",
      title: "distilgpt2",
      description: "Xenova/distilgpt2 quantized to 8bit",
      tasks: ["TextGenerationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "text-generation",
        modelPath: "Xenova/distilgpt2",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/LaMini-Flan-T5-783M:q8",
      title: "LaMini-Flan-T5-783M",
      description: "Xenova/LaMini-Flan-T5-783M quantized to 8bit",
      tasks: ["TextGenerationTask", "TextRewriterTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "text2text-generation",
        modelPath: "Xenova/LaMini-Flan-T5-783M",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/LaMini-Flan-T5-783M:q8",
      title: "LaMini-Flan-T5-783M",
      description: "Xenova/LaMini-Flan-T5-783M quantized to 8bit",
      tasks: ["TextGenerationTask", "TextRewriterTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "text2text-generation",
        modelPath: "Xenova/LaMini-Flan-T5-783M",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Falconsai/text_summarization:q8",
      title: "text_summarization",
      description: "Falconsai/text_summarization quantized to 8bit",
      tasks: ["TextSummaryTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "summarization",
        modelPath: "Falconsai/text_summarization",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/nllb-200-distilled-600M:q8",
      title: "nllb-200-distilled-600M",
      description: "Xenova/nllb-200-distilled-600M quantized to 8bit",
      tasks: ["TextTranslationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "translation",
        modelPath: "Xenova/nllb-200-distilled-600M",
        languageStyle: "FLORES-200",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/m2m100_418M:q8",
      title: "m2m100_418M",
      description: "Xenova/m2m100_418M quantized to 8bit",
      tasks: ["TextTranslationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "translation",
        modelPath: "Xenova/m2m100_418M",
        languageStyle: "ISO-639",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/m2m100_418M:q8",
      title: "m2m100_418M",
      description: "Xenova/m2m100_418M quantized to 8bit",
      tasks: ["TextTranslationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "translation",
        modelPath: "Xenova/m2m100_418M",
        languageStyle: "ISO-639",
        dType: "q8",
      },
      metadata: {},
    },
    {
      model_id: "onnx:Xenova/mbart-large-50-many-to-many-mmt:q8",
      title: "mbart-large-50-many-to-many-mmt",
      description: "Xenova/mbart-large-50-many-to-many-mmt quantized to 8bit",
      tasks: ["TextTranslationTask"],
      provider: HF_TRANSFORMERS_ONNX,
      providerConfig: {
        pipeline: "translation",
        modelPath: "Xenova/mbart-large-50-many-to-many-mmt",
        languageStyle: "ISO-639_ISO-3166-1-alpha-2",
        dType: "q8",
      },
      metadata: {},
    },
  ];

  for (const model of onnxModels) {
    await getGlobalModelRepository().addModel(model);
  }
}
